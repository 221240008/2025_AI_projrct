# 基于GUI-Owl的主动监视与策略驱动的GUI多智能体系统 —— 以微信app为例

## 一、项目背景与概述

在当前“智能体浪潮”推动下，通用 GUI 自动化方案（如 ShowUI、MobileAgent 等）已经能够实现从自然语言到界面操作的端到端控制，为用户提供了“可语言操控的界面”。然而，这类系统的核心特点依然是**以用户指令为中心**：用户通过语言输入描述目标，模型解析后生成对应的界面操作。其本质是一种 **“语言驱动的即时交互”** 机制——高效但仍需频繁的用户介入，且多数算法在复杂任务上的效率远不及用户操作GUI的效率。

随着个人与企业在社交、办公等平台上的信息管理需求愈加复杂，用户在这些场景中面临的负担已不再仅仅是“操作繁琐”，更是“信息持续涌入、处理流程被动”。在这种背景下，单纯依靠“指令→执行”的范式，已难以满足持续化、自主化的信息管理诉求。

相反，在多模态大模型之前，多数基于OCR的GUI助手项目（游戏助手、网站监控等）往往采用的是**自托管的交互范式**，即用户提供某种规则或行为逻辑，GUI助手相应地进行具体行为的执行、GUI状态的监控与提取等行为。这种交互范式相较于用户用自然语言操作GUI的范式更有效率，应用场景也更广泛。

本项目正是基于这一思考提出的：我们希望探索一种**Agent 托管 GUI 的新型智能体范式**——让系统不再等待用户命令，而是在预定义规则与大模型推理的双重支撑下，持续监听界面变化、判断事件优先级、并自主完成相应的操作闭环。通过这种方式，用户不再是“命令输入者”，而是“策略设定者”；GUI界面不再是交互对象，是**被智能体托管的任务环境**。

从项目实现的角度而言，基于GUI-Owl的Mobile Agent v3框架已经实现了多智能体互作的GUI操作系统，这显示了其基底模型GUI-Owl具有GUI识别、判断与操纵的多项潜力。本项目希望通过从模型GUI-Owl出发，设计多种Agent角色，实现一个主动监视-按规则解析-自动UI操作的Agents托管系统，首先在微信环境上进行相关实验，探索如何在实际环境中真正意义上减轻用户的操作负担。

项目的核心目标可概括为以下三方面：
1. **主动监视与界面语义识别**
基于 GUI-Owl 的视觉理解能力与周期性截屏机制，实现对微信界面的持续检测与语义解析。
系统定时截取当前UI画面，通过模型提取界面元素（如消息列表、联系人、按钮等）并转换为结构化文本与状态信息，为后续策略判断提供稳定输入。
2. **策略驱动与自调用决策模块**
结合 LangChain 框架实现规则引擎与Prompt模板机制，用户可定义自动化托管策略（如“检测到未读工作消息→自动回复”）；
当模型解析结果满足触发条件时，系统会自主调用任务规划流程，通过大模型推理生成对应的操作计划，实现主动决策与自调用执行触发。
3. **GUI层自动执行与状态闭环管理**
基于 GUI-Owl 的操作生成能力与轻量化执行接口，自动完成点击、输入、滑动等动作；
执行后系统将更新界面状态并记录于Prompt Memory模块中，实现“监视—判断—执行—再监视”的自治循环，形成持续化的托管执行闭环，从而在用户无需干预的情况下实现智能化任务代行。

与传统 ShowUI 等通用 GUI Agent 相比，本项目不再仅仅关注“能否识别界面、执行操作”，而更重视“谁在驱动操作、驱动依据是什么”。其技术创新点在于：
- **执行权的转移**：从用户即时指令驱动转向 Agent 持续托管执行；  
- **决策层的拆解与融合**：以规则引擎管理确定性任务，以大模型补充语义与生成性决策；  
- **交互范式的演化**：从“语言+界面”到“状态+策略”的持续代理机制。

通过这一探索，我们希望构建出一种更贴近实际使用场景的智能体体系：  
一个能够在后台持续运行、理解信息内容、遵循用户策略、并以 GUI 层操作为输出手段的托管式 Agent。  

## 二、系统任务与功能拆解

---

在明确项目目标之后，本系统的功能设计将围绕“**主动感知—智能判断—自动执行**”这三个核心能力来展开。  
系统的每一项功能都朝着“代理自治”的方向设计，目标是让智能体在无人干预的情况下持续运行，能够理解界面的变化，并依据策略自主采取行动。  
具体来说，系统功能可以分为以下三个部分：**监视功能、动作功能、判断与记忆功能**。

### （1）主动监视功能：持续理解界面与感知状态

主动监视功能是整个系统感知能力的基础，负责在后台对应用界面进行周期性的采样和语义分析。与传统的“按需操作”方式不同，本系统的监视模块强调持续监听，通过周期性截屏让智能体能够像真人用户一样，实时“看到”界面的动态变化。

它的主要工作包括：  
- **界面截取与元素提取**：定期捕捉当前界面画面，识别出界面中的可交互元素、文本内容以及状态信息；  
- **语义层级化描述**：把原始视觉信息转化为结构化的界面语义图谱，为后续的判断规则提供标准输入；  
- **事件变化检测**：通过对比前后界面状态，识别出关键事件（比如新消息出现、页面跳转、按钮状态变化等），从而触发系统的后续决策流程。

借助这一功能，系统获得了“持续感知外部环境”的能力，后续的判断和执行不再依赖用户手动输入，而是基于GUI的实时变化自动驱动。

### （2）动作执行功能：将策略转化为界面控制

动作功能是智能体与外部界面交互的主要手段，负责把高层决策和自然语言指令转化为实际可执行的 GUI 操作。  
其目标是构建一个“既能理解语义，又能执行操作”的通用中间层，让大模型的输出可以直接对应到具体的界面交互行为。

该功能的主要职责包括：  
- **指令解析与动作映射**：将自然语言描述的操作计划，转化为标准化的 GUI 指令（例如点击、输入、滑动等）。

这个功能是GUI智能体的核心功能，本项目希望借助通用GUI智能体的已有实现，完成从自然语言和GUI图像到GUI动作的端到端映射。

### （3）判断与记忆功能：规则触发、策略生成与状态积累

判断与记忆功能是系统的决策中枢，负责将感知结果与用户预设的策略结合起来，生成可执行的智能决策，并维护系统的运行记忆。  
它既是系统的“思考中心”，也是实现长期任务管理和上下文理解的关键。

该功能主要包括三个方面：  
- **规则判断机制**：根据用户设定的托管规则，对监视模块提供的界面状态进行条件匹配，判断是否要触发自动响应；  
- **策略推理与生成**：一旦触发条件满足，系统会调用大模型进行高层语义推理，生成符合当前上下文的自然语言策略；  
- **运行记忆与状态追踪**：持续记录界面状态的变化、执行历史以及策略决策结果，构建系统内部的记忆结构，为后续判断提供上下文支持。

通过“规则 + 记忆”的协同作用，系统能够在不断变化的交互环境中保持行为逻辑的一致性，逐步实现自我学习与行为稳定。

总结来看，本系统希望通过实现**监视、动作、判断与记忆**三大功能，构建了一个能够自我感知、自主决策并自动执行的智能体体系。  它不再依赖用户持续下达指令，而是可以在后台自主运行、理解任务语义、遵循策略规则，并通过界面操作完成任务闭环，最终实现真正意义上的**GUI 托管式智能代理**。



## 三、技术路线分析

---

在完成系统功能的总体设计后，接下来的关键任务是**明确各功能模块之间的协作机制与技术路径**。  
前一部分所定义的“监视—判断—执行”三项核心功能，构成了系统的行为逻辑框架；而要让这些功能在真实环境中稳定协同，就必须通过合理的体系架构将其连接起来。  

因此，考虑到目前已有相关功能的实现，本项目希望借助开源模型GUI-Owl完成智能体相关任务，在总体设计上采用了一个 **分层化、闭环式的系统结构**——即 **用户规则与决策层 — GUI-Owl 智能体层 — 实际 GUI 层**。这种三层体系不仅清晰地划分了感知、决策与执行的职责边界，也为多智能体的交互与扩展提供了技术基础。  

整体思路以“感知—决策—执行—反馈”为主线，通过 GUI-Owl 的视觉理解与操作生成能力，将自然语言规则驱动的高层逻辑与底层界面交互有机结合，形成闭环式的智能操作系统。

### （1）用户规则与决策层：语义理解与策略生成
该层是系统的逻辑中枢，负责直接理解用户以自然语言定义的操作意图，并在界面状态发生变化时进行决策。

实现上，将基于 LangChain 框架构建一个统一的语义理解与决策引擎。其核心是利用LLM的深层推理能力，实现“状态语义解析 + 上下文决策”的一体化判断机制。

运行流程描述如下：

当下层传入新的界面语义描述后，该描述将与用户预设的自然语言规则共同构成一个完整的决策上下文，直接输入给大型语言模型。

LLM将基于其对界面状态和用户规则的统一语义理解，进行综合判断。例如，当模型理解到当前是“工作群发来未读消息”这一状态，并结合用户“重要消息需及时回复”的规则时，将直接生成高层的策略方案，如“通过聊天窗口回复消息：告知已收到，请稍后处理。”。

最终输出的是基于深度语义理解的自然语言命令规划，并向下传递给执行代理。

这种一体化决策结构，依赖模型对自然语言的深层理解，从而在保证语义精准的同时，赋予了系统应对复杂和模糊语境的强大灵活性。唯一需要多次测试的技术难点是调整大语言模型提示词，使其输出的自然语言指令符合GUI-Owl所需要的提示粒度。

### （2）GUI-Owl 智能体层：感知、解析与执行中枢  

该层是连接人类规则与实际界面的桥梁，由 **Monitor（监视代理）** 与 **Executor（执行代理）** 两个子模块组成，分别承担“上行感知”与“下行操作”的任务。  

#### ① Monitor 监视代理（上行感知流）  
- 通过 ADB 截屏或模拟器流定时获取微信界面图像，采样周期可在 1–3 秒内灵活设定。  
- 调用 **GUI-Owl 感知模型**，输出界面结构化描述，包括：  
  - 元素层级结构（layout tree）；  
  - 文本内容与视觉坐标；  
  - 状态标签（可交互、未读、选中等）。  
- 生成的语义结果将转化为 JSON 格式，向上传递至用户规则层，用于触发规则判断与策略生成。

#### ② Executor 执行代理（下行操作流）  
- 接收来自用户规则层的自然语言操作指令。  
- 调用 **GUI-Owl 操作生成接口**，将其转化为标准化 GUI 指令，如：  
  - `click(element_id=msg_textbox)` → 点击输入框；  
  - `type("已收到")` → 文本输入；  
  - `swipe(direction="up")` → 滑动操作。  
- 指令通过模拟触控框架（如 AndroidViewClient 或 uiautomator2）下发至实际设备，实现对界面的直接操控。  
- 每次操作后对比操作前后状态是否符合预期；若偏差存在，则触发回退或重新规划，保证执行的闭环稳定性。

### （3）实际 GUI 层：环境接口与执行反馈  

该层代表系统直接作用的外部环境——**真实的微信 GUI 界面**或其等价模拟环境。  
所有的截图采集、元素定位与触控操作均在此层完成。  
它既是感知数据的来源，也是动作执行的落点，构成系统的“物理世界”。  

在执行层面，该界面通过 ADB 或自动化框架提供基础交互通道，确保智能体层可在无需修改应用本体的情况下实现完整操作。  
通过周期性反馈机制，GUI 层的变化会被重新捕获并上传，形成从界面到规则再回到界面的 **全程闭环反馈链**。

### （4）总体路线总结  

综上，本项目的技术路线以 **GUI-Owl模型的感知与操作能力** 为核心支撑，  
以三层结构实现从高层语义决策到底层界面执行的完整闭环：  

 **实际 GUI 层  →（实际GUI层的周期性信息） → GUI-Owl 智能体层 →（结构性GUI信息） → 用户规则与决策层 →（自然语言指令）→ GUI-Owl 智能体层 →（结构化动作）→ 实际 GUI 层**

这种“自上而下决策、自下而上感知”的体系既保证了工程可控性，又保留了模型驱动的灵活性，为后续的性能优化与扩展性研究奠定了坚实基础。

---

## 四、创新点分析

---

相较于现有的通用 GUI 自动化与语言控制类智能体方案，本项目的探索虽仍处于原型阶段，但在体系理念与实现路径上尝试了一些新的结合与延展。总体来说，其创新性主要体现在“执行逻辑的转向”与“决策机制的融合”两方面。

首先，在交互范式上，本项目实现了从“用户指令驱动”向“Agent 托管执行”的转变。传统的 GUI 控制系统往往以即时命令为核心，用户输入指令后模型才执行相应操作。而本系统将 **持续监视** 与 **规则触发** 引入其中，使智能体具备主动感知与响应的能力，能够在用户未直接发起指令的情况下，根据策略自动完成任务。这一变化虽然依赖简单规则，但体现了从“交互工具”到“代理执行者”的过渡思路。

其次，在决策机制上，本项目结合了**确定性规则引擎与生成式推理模型**。通过 LangChain 框架的结构化设计，系统能在面对明确事件时快速执行，同时利用大模型在语义理解上的灵活性处理泛化情境。这样的“双轨式判断体系”使系统既具确定性保障，又保留开放性与上下文感知能力，为实践中应对多样界面状态提供了可能。

再次，在闭环机制方面，项目强调了 **监视–执行–反馈–学习** 的持续循环。通过记录与记忆模块，系统可在有限范围内积累运行经验，可在后续项目内实现对策略触发与执行逻辑进行微调，逐步形成自我完善的运行闭环。这一特性使智能体不再是一次性脚本，而是一个有“演化可能”的自治系统。

总体而言，本项目的创新并非体现在算法突破，而在于如何将多种已有能力以统一的结构串联起来，使“界面理解—操作规划—主动决策”形成连续通路。它更像是一种面向未来 GUI 智能体形态的**小尺度尝试**——在保持可解释性与操作可控的前提下，探索更具“持续性、策略性、自治性”的人机交互模式。

---

## 五、可行性分析与工作安排

本项目的总体目标是构建一个轻量级智能体，能够在微信界面中实现“主动监视—策略触发—自动操作”的完整流程。

### （1）可行性分析

从**技术可行性**来看，我们所需的核心组件都已相当成熟——无论是GUI-Owl模型、LangChain框架，还是ADB截屏与uiautomator2操控接口，都有完善的开源生态支持。通过适当的模型微调和参数配置，我们完全能够实现微信界面的结构化识别和动作生成，技术上不存在明显的障碍。

就**实现复杂度**而言，系统的整体架构层次分明，各模块之间的接口标准化程度高。虽然策略触发逻辑和界面变化检测确实有些挑战，但我们可以通过分阶段的原型验证来逐步优化，这个问题是可解的。

在**运行环境**方面，系统可以在本地设备或虚拟机环境中运行，不需要对微信应用做任何底层修改，这让开发和测试的风险都处于可控范围。

另外，我们采用的三层架构——监视、决策、执行——可以并行开发和调试，这种设计让项目在较短时间内完成最小可用版本变得切实可行。

### （2）工作安排

考虑到项目规模和目标，我们将整个周期规划为**两个月（约八周）**，分为四个主要阶段：

- **第一阶段（第1–2周）：需求确认与环境搭建**  
  明确系统边界和功能优先级，搭建开发环境（包括Android模拟器、微信测试账号等）；  
  同时完成GUI-Owl模型和LangChain框架的安装与初步调试。

- **第二阶段（第3–4周）：监视与感知模块实现**  
  建立周期性的截屏和界面状态解析流程，利用GUI-Owl输出结构化的元素描述；  
  实现基础的界面变化检测和状态记录功能，完成一个简易的展示界面。

- **第三阶段（第5–6周）：策略决策与操作执行模块开发**  
  搭建规则引擎模板和Prompt调用链路，实现基于规则的操作触发机制；  
  连接执行接口（uiautomator2或AndroidViewClient），完成自动点击和输入的动作闭环。

- **第四阶段（第7–8周）：反馈与调优整合**  
  构建记忆模块和日志系统，实现执行结果的回写和状态追踪；  
  进行全流程的集成测试，优化触发逻辑和界面识别准确率，最后整理实验结果和项目文档。

总的来说，这个项目技术基础扎实、模块划分清晰、风险可控，通过两个月的分阶段推进，我们完全能够完成基础功能的验证。如果时间允许，后续还可以在界面识别率、决策准确性和策略多样性方面做进一步的优化，为将来扩展到更多应用场景奠定基础。


## 六、实验结果演示与结论

---

TBD

---

## 七、参考与AI使用声明

---

**关于开源项目**

本项目主要使用的开源项目为GUI-Owl多模态模型，但如前所述本项目不旨在重新微调一个GUI特化的多模态模型，而是希望借助已有通用GUI模型的端到端功能与多智能体潜力探讨新的交互范式，因此理论上任何一个在GUI任务中表现较好的多模态模型都可以代替GUI-Owl。

选择GUI-Owl作为核心的原因有二：

1. 其为开源模型，参数量较小（7B，可运行在16GB显存上），便于本地部署与调试。

2. 其是目前部分GUI任务的SOTA，相关表现良好，也是Mobile Agent v3的基座模型，已有验证过的多智能体场景。

**关于参考文献**

- Hong W, Wang W, Lv Q, Xu J, Yu W, Ji J, Wang Y, Wang Z, Dong Y, Ding M, Tang J. *CogAgent: A Visual Language Model for GUI Agents*, 2023.  
- Lin K Q, Li L, Gao D, Yang Z, Wu S, Bai Z, Lei W, Wang L, Shou M Z. *ShowUI: One Vision-Language-Action Model for GUI Visual Agent*, 2024.  
- Ye J, Zhang X, Xu H, Liu H, Wang J, Zhu Z, Zheng Z, Gao F, Cao J, Lu Z, et al. *Mobile-Agent-v3: Foundamental Agents for GUI Automation*, 2025.  
- Lu Z, Ye J, Tang F, Shen Y, Xu H, Zheng Z, Lu W, Yan M, Huang F, Xiao J, et al. *UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning*, 2025.  
- Wanyan Y, Zhang X, Xu H, Liu H, Wang J, Ye J, Kou Y, Yan M, Huang F, Yang X, et al. *Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation*, 2025.  
- Liu H, Zhang X, Xu H, Wanyan Y, Wang J, Yan M, Zhang J, Yuan C, Xu C, Hu W, Huang F. *PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC*, 2025.  
- Wang Z, Xu H, Wang J, Zhang X, Yan M, Zhang J, Huang F, Ji H. *Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks*, 2025.  
- Wang J, Xu H, Jia H, Zhang X, Yan M, Shen W, Zhang J, Huang F, Sang J. *Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration*, 2024.  
- Wang J, Xu H, Ye J, Yan M, Shen W, Zhang J, Huang F, Sang J. *Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception*, 2024.

**关于生成式AI的使用**

本报告在初期技术草稿的基础上由AI整理完成框架，主要内容由个人完成。

文中使用的配图由AI生成。

---

